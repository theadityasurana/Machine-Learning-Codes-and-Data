{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmVnGENiNpJl",
        "outputId": "c18721cc-671e-4f65-a0f3-9de5c4d2be91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes:  0.9333333333333333\n",
            "Precision of Gaussian Naive Bayes:  0.9352007469654529\n",
            "Recall of Gaussian Naive Bayes:  0.9333333333333333\n",
            "F1-Score of Gaussian Naive Bayes:  0.933615520282187\n",
            "Accuracy of Decision Tree Classifier:  0.9555555555555556\n",
            "Precision of Decision Tree Classifier:  0.9555555555555556\n",
            "Recall of Decision Tree Classifier:  0.9555555555555556\n",
            "F1-Score of Decision Tree Classifier:  0.9555555555555556\n",
            "Accuracy of Support Vector Machine:  1.0\n",
            "Precision of Support Vector Machine:  1.0\n",
            "Recall of Support Vector Machine:  1.0\n",
            "F1-Score of Support Vector Machine:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# import the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# splitting X and y into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "\tX, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# GAUSSIAN NAIVE BAYES\n",
        "gnb = GaussianNB()\n",
        "# train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "# make predictions\n",
        "gnb_pred = gnb.predict(X_test)\n",
        "# print the accuracy\n",
        "print(\"Accuracy of Gaussian Naive Bayes: \",\n",
        "\taccuracy_score(y_test, gnb_pred))\n",
        "# print other performance metrics\n",
        "print(\"Precision of Gaussian Naive Bayes: \",\n",
        "\tprecision_score(y_test, gnb_pred, average='weighted'))\n",
        "print(\"Recall of Gaussian Naive Bayes: \",\n",
        "\trecall_score(y_test, gnb_pred, average='weighted'))\n",
        "print(\"F1-Score of Gaussian Naive Bayes: \",\n",
        "\tf1_score(y_test, gnb_pred, average='weighted'))\n",
        "\n",
        "# DECISION TREE CLASSIFIER\n",
        "dt = DecisionTreeClassifier(random_state=0)\n",
        "# train the model\n",
        "dt.fit(X_train, y_train)\n",
        "# make predictions\n",
        "dt_pred = dt.predict(X_test)\n",
        "# print the accuracy\n",
        "print(\"Accuracy of Decision Tree Classifier: \",\n",
        "\taccuracy_score(y_test, dt_pred))\n",
        "# print other performance metrics\n",
        "print(\"Precision of Decision Tree Classifier: \",\n",
        "\tprecision_score(y_test, dt_pred, average='weighted'))\n",
        "print(\"Recall of Decision Tree Classifier: \",\n",
        "\trecall_score(y_test, dt_pred, average='weighted'))\n",
        "print(\"F1-Score of Decision Tree Classifier: \",\n",
        "\tf1_score(y_test, dt_pred, average='weighted'))\n",
        "\n",
        "# SUPPORT VECTOR MACHINE\n",
        "svm_clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "# train the model\n",
        "svm_clf.fit(X_train, y_train)\n",
        "# make predictions\n",
        "svm_clf_pred = svm_clf.predict(X_test)\n",
        "# print the accuracy\n",
        "print(\"Accuracy of Support Vector Machine: \",\n",
        "\taccuracy_score(y_test, svm_clf_pred))\n",
        "# print other performance metrics\n",
        "print(\"Precision of Support Vector Machine: \",\n",
        "\tprecision_score(y_test, svm_clf_pred, average='weighted'))\n",
        "print(\"Recall of Support Vector Machine: \",\n",
        "\trecall_score(y_test, svm_clf_pred, average='weighted'))\n",
        "print(\"F1-Score of Support Vector Machine: \",\n",
        "\tf1_score(y_test, svm_clf_pred, average='weighted'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJ3pB-CAPG9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4w1_PiFLPGvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass classification using scikit-learn"
      ],
      "metadata": {
        "id": "DCqMtqU2PHPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass classification is a popular problem in supervised machine learning.\n",
        "\n",
        "Problem – Given a dataset of m training examples, each of which contains information in the form of various features and a label. Each label corresponds to a class, to which the training example belongs. In multiclass classification, we have a finite set of classes. Each training example also has n features.\n",
        "\n",
        "For example, in the case of identification of different types of fruits, “Shape”, “Color”, “Radius” can be featured, and “Apple”, “Orange”, “Banana” can be different class labels.\n",
        "\n",
        "In a multiclass classification, we train a classifier using our training data and use this classifier for classifying new examples.\n",
        "\n",
        "Aim of this article – We will use different multiclass classification methods such as, KNN, Decision trees, SVM, etc. We will compare their accuracy on test data. We will perform all this with sci-kit learn (Python). For information on how to install and use sci-kit learn, visit http://scikit-learn.org/stable/\n",
        "\n",
        "Approach –  \n",
        "\n",
        "Load dataset from the source.\n",
        "Split the dataset into “training” and “test” data.\n",
        "Train Decision tree, SVM, and KNN classifiers on the training data.\n",
        "Use the above classifiers to predict labels for the test data.\n",
        "Measure accuracy and visualize classification."
      ],
      "metadata": {
        "id": "65BhZeXUPIcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree classifier – A decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree. On the root and each of the internal nodes, a question is posed and the data on that node is further split into separate records that have different characteristics. The leaves of the tree refer to the classes in which the dataset is split. In the following code snippet, we train a decision tree classifier in scikit-learn.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7DwqguTMPTzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# X -> features, y -> label\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# dividing X, y into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "\n",
        "# training a DescisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)\n",
        "dtree_predictions = dtree_model.predict(X_test)\n",
        "\n",
        "# creating a confusion matrix\n",
        "cm = confusion_matrix(y_test, dtree_predictions)\n"
      ],
      "metadata": {
        "id": "fbfT87vVPICA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM (Support vector machine) classifier –\n",
        "SVM (Support vector machine) is an efficient classification method when the feature vector is high dimensional. In sci-kit learn, we can specify the kernel function (here, linear). To know more about kernel functions and SVM refer – Kernel function | sci-kit learn and SVM."
      ],
      "metadata": {
        "id": "TDUrpCjcPGfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# X -> features, y -> label\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# dividing X, y into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "\n",
        "# training a linear SVM classifier\n",
        "from sklearn.svm import SVC\n",
        "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
        "svm_predictions = svm_model_linear.predict(X_test)\n",
        "\n",
        "# model accuracy for X_test\n",
        "accuracy = svm_model_linear.score(X_test, y_test)\n",
        "\n",
        "# creating a confusion matrix\n",
        "cm = confusion_matrix(y_test, svm_predictions)\n"
      ],
      "metadata": {
        "id": "Hb_4lFmuPbnb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN (k-nearest neighbors) classifier – KNN or k-nearest neighbors is the simplest classification algorithm. This classification algorithm does not depend on the structure of the data. Whenever a new example is encountered, its k nearest neighbors from the training data are examined. Distance between two examples can be the euclidean distance between their feature vectors. The majority class among the k nearest neighbors is taken to be the class for the encountered example."
      ],
      "metadata": {
        "id": "cvfj2A5aPeTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# X -> features, y -> label\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# dividing X, y into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "\n",
        "# training a KNN classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train)\n",
        "\n",
        "# accuracy on X_test\n",
        "accuracy = knn.score(X_test, y_test)\n",
        "print(accuracy)\n",
        "\n",
        "# creating a confusion matrix\n",
        "knn_predictions = knn.predict(X_test)\n",
        "cm = confusion_matrix(y_test, knn_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzquYEO1Pg7D",
        "outputId": "cad9d002-fc8d-4b37-9b86-7abdf9306fbd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes classifier – Naive Bayes classification method is based on Bayes’ theorem. It is termed as ‘Naive’ because it assumes independence between every pair of features in the data. Let (x1, x2, …, xn) be a feature vector and y be the class label corresponding to this feature vector.\n",
        "Applying Bayes’ theorem,\n",
        "\n",
        "Naive Bayes classifier equation\n",
        "\n",
        "Since, x1, x2, …, xn are independent of each other,  \n",
        "\n",
        "Naive Bayes classifier equation\n",
        "\n",
        "Inserting proportionality by removing the P(x1, …, xn) (since it is constant).\n",
        "\n",
        "Naive Bayes classifier equation\n",
        "\n",
        "Therefore, the class label is decided by,\n",
        "\n",
        "Naive Bayes classifier equation\n",
        "\n",
        "P(y) is the relative frequency of class label y in the training dataset.\n",
        "In the case of the Gaussian Naive Bayes classifier, P(xi | y) is calculated as,\n",
        "\n",
        "Naive Bayes classifier equation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A7GwLVzEPqIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# X -> features, y -> label\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# dividing X, y into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "\n",
        "# training a Naive Bayes classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB().fit(X_train, y_train)\n",
        "gnb_predictions = gnb.predict(X_test)\n",
        "\n",
        "# accuracy on X_test\n",
        "accuracy = gnb.score(X_test, y_test)\n",
        "print(accuracy)\n",
        "\n",
        "# creating a confusion matrix\n",
        "cm = confusion_matrix(y_test, gnb_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn8rJX49Pt2J",
        "outputId": "b143c391-b753-4b0e-d356-fe2c438fdf59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R69OcpjfoH-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J8r5unDnoIUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jlvj8abhoInk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azeYImofoJME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ACTWk2bToJyY"
      }
    }
  ]
}